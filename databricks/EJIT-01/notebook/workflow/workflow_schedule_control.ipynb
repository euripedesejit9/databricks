{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33514277-3d92-4315-bd5c-82dc0f1ae5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../utils/commom_libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af21801-a278-42f5-971c-e3ddfd947128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../utils/yaml_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583b86b-d88f-408f-871d-5f172bd67b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../connector/databricks_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e0c694-5480-4b67-904b-611448a6af75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WorkflowScheduleControl:\n",
    "\n",
    "    def __init__(self, country:str):\n",
    "        self.country = country.lower()\n",
    "        self.logger_scan = LoggerScan()\n",
    "        self.yaml_manager = YamlManager(project_name=\"EJIT-01\")\n",
    "        self.adb_conn = DatabricksApi(scope=\"DataEngTeam\", base_url=\"adb-url\", secret_key= \"adb-token-api\")\n",
    "        self.parameter_sent = {}\n",
    "\n",
    "    @property\n",
    "    def generate_schedule_schema(self):\n",
    "        return T.StructType([\n",
    "            T.StructField('country', T.StringType(), True),\n",
    "            T.StructField('partner_id', T.StringType(), True),\n",
    "            T.StructField('partner_name', T.StringType(), True),\n",
    "            T.StructField('pipeline', T.StringType(), True),\n",
    "            T.StructField('is_enable_pipeline', T.BooleanType(), True),\n",
    "            T.StructField('pipeline_schedule', T.ArrayType(T.StringType(), True), True),\n",
    "            T.StructField('job_id', T.StringType(), True),\n",
    "            T.StructField('margin_time', T.LongType(), True),\n",
    "            T.StructField('target_date', T.DateType(), True),\n",
    "            T.StructField('load_ts', T.TimestampType(), True)\n",
    "        ])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def generate_schema_log_table(self):\n",
    "        return T.StructType([\n",
    "            T.StructField('country', T.StringType(), True),\n",
    "            T.StructField('partner_id', T.StringType(), True),\n",
    "            T.StructField('partner_name', T.StringType(), True),\n",
    "            T.StructField('pipeline_name', T.StringType(), True),\n",
    "            T.StructField('log_date', T.DateType(), True),\n",
    "            T.StructField('idempotency_token', T.StringType(), True),\n",
    "            T.StructField('job_id', T.StringType(), True),\n",
    "            T.StructField('run_id', T.LongType(), True),\n",
    "            T.StructField('schedule_yaml_time', T.StringType(), True),\n",
    "            T.StructField('execution_at', T.StringType(), True),\n",
    "            T.StructField('job_parameters', T.MapType(T.StringType(), T.StringType(), True), True)\n",
    "        ])\n",
    "      \n",
    "\n",
    "    @property\n",
    "    def get_yaml_files(self) -> List[Dict]:\n",
    "        yaml_files = []\n",
    "        job_to_run_list = []\n",
    "        all_yaml_files = self.yaml_manager.get_all_files\n",
    "\n",
    "        for yaml_file_path in all_yaml_files:            \n",
    "            file_name = yaml_file_path.split(os.sep)[9].removesuffix(\".yaml\")\n",
    "            if self.country in file_name:\n",
    "                yaml_files.append(file_name)\n",
    "\n",
    "        for path_yaml_file in yaml_files:\n",
    "            data = self.yaml_manager.load_yaml_file(path_yaml_file)\n",
    "            for partner in (data.keys()):\n",
    "                for pipeline in data[partner][\"schedule\"][\"pipelines\"].keys():\n",
    "                    job_to_run_list.append({\n",
    "                        \"country\": (path_yaml_file.split(\"_\")[0]),\n",
    "                        \"partner_name\":data[partner][\"name\"],\n",
    "                        \"partner_id\": partner,\n",
    "                        \"pipeline\": pipeline,\n",
    "                        \"margin_time\": data[partner][\"schedule\"][\"margin_time\"],\n",
    "                        \"is_enable_pipeline\": data[partner][\"schedule\"][\"pipelines\"][pipeline][\"enable\"],\n",
    "                        \"job_id\":data[partner][\"schedule\"][\"pipelines\"][pipeline][\"job_id\"],\n",
    "                        \"pipeline_schedule\": data[partner][\"schedule\"][\"pipelines\"][pipeline][\"trigger_at\"],\n",
    "                        \"load_ts\": dt.datetime.now(),\n",
    "                        \"target_date\": dt.date.today()\n",
    "                    })\n",
    "        return  job_to_run_list\n",
    "\n",
    "    @property\n",
    "    def get_schedule_pipeline(self) -> DataFrame:\n",
    "        yaml_dict = self.get_yaml_files\n",
    "        schema_yaml_schedule = self.generate_schedule_schema\n",
    "\n",
    "        config_df = spark.createDataFrame(yaml_dict, schema_yaml_schedule) \\\n",
    "            .withColumn(\"pipeline_schedule\", F.explode(F.col(\"pipeline_schedule\")))\n",
    "            \n",
    "        try:\n",
    "            self.margin_time = config_df.select(\"margin_time\").collect()[0][\"margin_time\"]\n",
    "        except Exception as e:\n",
    "            self.margin_time = 0\n",
    "\n",
    "        return config_df \\\n",
    "            .withColumn(\"past_utc_date\", F.lit(dt.datetime.utcnow() - dt.timedelta(minutes= self.margin_time))) \\\n",
    "            .withColumn(\"current_utc_date\", F.lit(dt.datetime.utcnow())) \\\n",
    "            .withColumn(\"future_utc_date\", F.lit(dt.datetime.utcnow() + dt.timedelta(minutes= self.margin_time))) \\\n",
    "            .withColumn(\"is_prepared_to_execute\", F.date_format(F.col(\"pipeline_schedule\"), \"HH:mm\").between(F.col(\"past_utc_date\"), F.col(\"future_utc_date\")))\n",
    "\n",
    "\n",
    "    def start_adb_job(self, parameter_payload: dict):\n",
    "        try:\n",
    "            self.logger_scan.info(f\"START Databricks API JOB .....\")\n",
    "            response_api = self.adb_conn.run_job(parameter_payload)\n",
    "            self.parameter_sent.update(response_api)\n",
    "            get_run_id = response_api[\"run_id\"]\n",
    "            self.logger_scan.info(f\"FINISHED Databricks API JOB with run_id {get_run_id}....\")\n",
    "        except Exception as e:\n",
    "            self.logger_scan.warning(f\"FAILED Databricks API JOB - {e}.\")\n",
    "\n",
    "\n",
    "    def trigger_job_schedule(self):\n",
    "        log_parameter_list = []\n",
    "        self.logger_scan.info(f\"GENERATE SCHEMA - on Yaml Files for a Temporary DF\")\n",
    "\n",
    "        schedule_info_df = self.get_schedule_pipeline \\\n",
    "            .filter(F.col(\"is_enable_pipeline\") == True) \\\n",
    "            .filter(F.col(\"is_prepared_to_execute\") == True)\n",
    "\n",
    "\n",
    "        while True:\n",
    "            if schedule_info_df.isEmpty():\n",
    "                self.logger_scan.info(f\"NO SCHEDULE - for RUN based on schedule from yaml check if it is enabled\")\n",
    "                break\n",
    "\n",
    "            self.logger_scan.info(f\"WE HAVE {schedule_info_df.count()} schedules prepared to Run in a interval of the margin_time: {self.margin_time} minutes\")\n",
    "\n",
    "            self.logger_scan.info(f\"GET PARAMETERS -  based on schedule from yaml\")\n",
    "            for row in schedule_info_df.collect():\n",
    "                parameters_data_dict = {\n",
    "                    \"job_id\": row['job_id'],\n",
    "                    \"job_parameters\": {\n",
    "                        \"country\" : row['country'],\n",
    "                        \"partner_name\" : row['partner_name'],\n",
    "                        \"pipeline_name\": row['pipeline'],\n",
    "                        \"environment\": \"uat\"\n",
    "                    }\n",
    "                }\n",
    "                self.logger_scan.info(f\"START PROCESS - for RUN job_id: {row['job_id']} based on schedule from yaml\")\n",
    "                self.start_adb_job(parameters_data_dict)\n",
    "\n",
    "                schedule_parameters = {\n",
    "                    \"country\": row['country'],\n",
    "                    \"partner_id\": row['partner_id'],\n",
    "                    \"partner_name\": row['partner_name'],\n",
    "                    \"pipeline_name\": row['pipeline'],\n",
    "                    \"log_date\" : dt.date.today() ,\n",
    "                    \"execution_at\" : dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"schedule_yaml_time\" : row['pipeline_schedule']\n",
    "                }\n",
    "                combined_parameters = {**schedule_parameters, **parameters_data_dict, **self.parameter_sent}\n",
    "                log_parameter_list.append(combined_parameters)\n",
    "                self.logger_scan.info(f\"\"\"FINISHED PROCESS - pipeline: {row['pipeline']}, job_id: {row['job_id']}, Schedule_Time: {row['pipeline_schedule']}, country: {row['country']} and partner: {row['partner_name']}\n",
    "                \"\"\")\n",
    "                \n",
    "            break\n",
    "        try:\n",
    "            self.logger_scan.info(f\"START SAVE LOG ON DELTA TABLE\")\n",
    "            spark.createDataFrame(log_parameter_list, self.generate_schema_log_table)\\\n",
    "                .write\\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"eng_team_log__workflow_job\") #add table from UnityCatalog \n",
    "            self.logger_scan.info(f\"FINISHED SAVE LOG ON eng_team.log__workflow_job successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger_scan.warning(f\"FAILED SAVE LOG eng_team.log__workflow_job - {e}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workflow_schedule_control",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
